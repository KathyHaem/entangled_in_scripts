{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30081b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast, AutoTokenizer, AutoConfig\n",
    "from transformers import XLMRobertaTokenizerFast\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155d4a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import notebook_utils as nbu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425f147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config and constants\n",
    "tokenizer_dir = \"../../../tokenizers\"\n",
    "\n",
    "languages = ('ar', 'tr', 'zh','el', 'es', 'en')\n",
    "multil_string = '-'.join(languages)\n",
    "all_languages = languages + (multil_string,)\n",
    "\n",
    "alphas = ('0.0', '0.25', '0.5', '0.75', '1.0')\n",
    "alphas_num = np.array([float(alpha) for alpha in alphas])\n",
    "NV_mono = 20000\n",
    "NV_multi = 120000\n",
    "\n",
    "tok_type = \"sp-unigram\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e112bb48",
   "metadata": {},
   "source": [
    "# Explore the tokenizer vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b7e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load token occurences\n",
    "token_stats = {}\n",
    "for alpha in alphas:\n",
    "    token_stats[alpha] = {}\n",
    "    for lang in all_languages:\n",
    "        NV = NV_mono if lang in languages else NV_multi\n",
    "        token_stats[alpha][lang] = nbu.get_token_frequencies(tokenizer_dir, tok_type, lang, alpha, NV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78325964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('unicode_blocks.tsv', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    unicode_blocks = list(reader)\n",
    "# convert hex strings to int\n",
    "unicode_blocks = [(int(block[0], 16), int(block[1], 16), block[2]) for block in unicode_blocks]\n",
    "len(unicode_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82355b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute character occurences from the token occurences\n",
    "from collections import Counter\n",
    "\n",
    "def count_stats(token_stats, process_token_fn, skip_tokens):\n",
    "    stats = {}\n",
    "    for alpha in token_stats:\n",
    "        stats[alpha] = {}\n",
    "        for lang in token_stats[alpha]:\n",
    "            stats[alpha][lang] = Counter()\n",
    "            for token, freq in token_stats[alpha][lang].items():\n",
    "                if token in skip_tokens:\n",
    "                    continue\n",
    "                # if token.startswith(\"<\"):\n",
    "                #     print(f\"Possibly special token: {token}\")\n",
    "                stats[alpha][lang].update()\n",
    "    return stats\n",
    "\n",
    "char_stats = count_stats(token_stats, lambda token: {char: freq for char in token}, skip_tokens=[\"<s>\", \"</s>\"])\n",
    "char_stats_unicode_blocks = count_stats(token_stats, lambda token: {char_to_block(char): freq for char in token}, skip_tokens=[\"<s>\", \"</s>\"])\n",
    "for alpha in alphas:\n",
    "    char_stats_unicode_blocks[alpha] = {}\n",
    "    for lang in all_languages:\n",
    "        char_stats_unicode_blocks[alpha][lang] = Counter()\n",
    "        for char, freq in char_stats[alpha][lang].items():\n",
    "            if token in [\"<s>\", \"</s>\"]:\n",
    "                continue\n",
    "            # if token.startswith(\"<\"):\n",
    "            #     print(f\"Possibly special token: {token}\")\n",
    "            char_stats_unicode_blocks[alpha][lang].update({nbu.get_unicode_block(letter): freq for letter in token})\n",
    "\n",
    "\n",
    "# for lang in languages: # languages = ('ar', 'tr', 'zh','el', 'es', 'en')\n",
    "#     vocab = mono_tokenizers[f\"alpha1.0\"][lang].get_vocab()\n",
    "#     tokens = list(vocab.keys())\n",
    "#     lang_stats = Counter()\n",
    "#     for token in tokens:\n",
    "#         for char in token:\n",
    "#             # print(ord(char))\n",
    "#             lang_stats[ord(char)] += 1\n",
    "#             all_stats[ord(char)] += 1\n",
    "\n",
    "#     # plot histplot of lang_stats using seaborn\n",
    "#     print(f\"Language: {lang}\")\n",
    "#     # print(max(lang_stats.keys()))\n",
    "#     # print(max(lang_stats.values()))\n",
    "#     # sns.set_theme(style=\"whitegrid\")\n",
    "#     # ax = sns.histplot(x=np.arange(num_utf8_chars), y=lang_stats, bins=100, log_scale=(False, False))\n",
    "#     # ax.set(xlabel='unicode code', ylabel='Number of characters', title=f\"Character frequency in {lang} vocabulary\")\n",
    "#     # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488921d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of characters in each block\n",
    "block_counts = [0] * len(unicode_blocks)\n",
    "for char in all_stats.keys():\n",
    "    for i, block in enumerate(unicode_blocks):\n",
    "        if char >= block[0] and char <= block[1]:\n",
    "            block_counts[i] += all_stats[char]\n",
    "            break\n",
    "# create examples of characters in each block\n",
    "block_examples = []\n",
    "for block in unicode_blocks:\n",
    "    all_used = [chr(i) for i in range(block[0], block[1]+1) if i in all_stats.keys()]\n",
    "    # random shuffle\n",
    "    random.shuffle(all_used)\n",
    "    block_examples.append(all_used[:10])\n",
    "\n",
    "for block, count, example in zip(unicode_blocks, block_counts, block_examples):\n",
    "    print(f\"{block[2]}, occurence {count} {example}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bc61d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "# barplot of block_counts using seaborn\n",
    "# skip unused blocks\n",
    "filter_n = 20\n",
    "filtered_block_names = [f\"{block[2]} ({''.join(ex)})\" for block, count, ex in zip(unicode_blocks, block_counts, block_examples) if count > filter_n]\n",
    "filtered_block_counts = list(filter(lambda x: x > filter_n, block_counts))\n",
    "\n",
    "# use Droid Sans and Droid Sans Fallback fonts\n",
    "# sns.set_theme(style=\"whitegrid\", )\n",
    "\n",
    "matplotlib.rcParams['font.family'] = ['DejaVu Sans', 'Droid Sans', 'Droid Sans Fallback', 'DejaVu Sans Mono', \n",
    "'Noto Sans Arabic', 'Noto Sans Devanagari', 'STIXNonUnicode', 'EB Garamond', 'Noto Sans Symbols2']\n",
    "# sns.set(font='Comic Sans MS')\n",
    "\n",
    "# suppress userwarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# enable user warning\n",
    "warnings.filterwarnings(\"default\")\n",
    "\n",
    "plt.figure(figsize=(10,20))\n",
    "#sns.set_theme(style=\"whitegrid\")\n",
    "ax = sns.barplot(y=filtered_block_names, x=filtered_block_counts, log=True)\n",
    "ax.set(ylabel='unicode block', xlabel='Number of characters', title=f\"Character frequency in all vocabulary\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9e57c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this cell we can search for a font that contains a specific character\n",
    "\n",
    "from fontTools.ttLib import TTFont\n",
    "import matplotlib.font_manager as mfm\n",
    "\n",
    "def char_in_font(unicode_char, font):\n",
    "    for cmap in font['cmap'].tables:\n",
    "        if cmap.isUnicode():\n",
    "            if ord(unicode_char) in cmap.cmap:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# uni_char =  u\"è¯‹\"\n",
    "uni_char = u\"\\N{SYRIAC LETTER SHIN}\"\n",
    "print(uni_char)\n",
    "\n",
    "font_info = [(f.fname, f.name) for f in mfm.fontManager.ttflist]\n",
    "\n",
    "for i, font in enumerate(font_info):\n",
    "    if char_in_font(uni_char, TTFont(font[0])):\n",
    "        print(font[0], font[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
