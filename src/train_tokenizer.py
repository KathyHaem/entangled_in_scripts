import torchimport loggingimport osimport argparseimport sysfrom transformers import set_seedimport jsonfrom tokenizers import Tokenizerfrom tokenizers.models import Unigram, BPEfrom tokenizers.trainers import UnigramTrainer, BpeTrainerfrom tokenizers.normalizers import BertNormalizerfrom tokenizers.pre_tokenizers import BertPreTokenizerfrom tokenizers.processors import TemplateProcessingset_seed(20)logging.info(torch.cuda.is_available())def save_tokenizer(tokenizer, out_dir, type, languages, vocab_size, alpha, lowercase):    """ Function saving vocab and arguments to the """    out_path = os.path.join(out_dir, type, '-'.join(languages),f"alpha-{alpha}_N-{vocab_size}" )    if lowercase:        out_path += '-lc'    os.makedirs(out_path, exist_ok=True)    logging.info(f"Saving tokenizer at {out_path}")    tokenizer.save(os.path.join(out_path, "tokenizer.json"))       with open(sys.argv[0], 'r') as cur_file:        cur_running = cur_file.readlines()    with open(os.path.join(out_path,'script.py'),'w') as log_file:        log_file.writelines(cur_running)    with open(os.path.join(out_path,'args.txt'),'w') as log_file:        log_file.writelines(sys.argv[1:])            # Saving vocab    with open(os.path.join(out_path,'vocab.json'), "w", encoding='utf-8') as outfile:        json.dump(dict(sorted(tokenizer.get_vocab().items(), key=lambda item: item[1])),                  outfile, indent=2, ensure_ascii=False)def main(args):    alpha = args.alpha    vocab_size = args.vocab_size    languages = args.languages    data_paths = args.data_list    lowercase = not args.cased        type = args.type    out_dir= args.out_dir    special_tokens=["[UNK]","[CLS]","[SEP]","[PAD]","[MASK]"]        if type == "unigram":        # TODO: check where to put special tokens        tokenizer = Tokenizer(Unigram(None))        trainer = UnigramTrainer(vocab_size=vocab_size, special_tokens=special_tokens)    elif type == "bpe":        tokenizer = Tokenizer(BPE(unk_token="[UNK]"))        trainer = BpeTrainer(vocab_size=vocab_size, special_tokens=special_tokens)    else:        raise ValueError(f"Unknown tokenizer type: {type}.")    tokenizer.normalizer = BertNormalizer(lowercase=lowercase)    tokenizer.pre_tokenizer = BertPreTokenizer()    tokenizer.post_processor = TemplateProcessing(        single="[CLS] $A [SEP]",        pair="[CLS] $A [SEP] $B:1 [SEP]:1",        special_tokens=[            ("[CLS]", special_tokens.index("[CLS]")),            ("[SEP]", special_tokens.index("[SEP]")),        ],    )        # Customize training    logging.info(f"Training tokenizer on:\n{data_paths}")    tokenizer.train(        data_paths,        trainer    )        save_tokenizer(tokenizer, out_dir, type, languages, vocab_size, alpha, lowercase)    logging.info("Done creating tokenizer")if __name__ == '__main__':    parser = argparse.ArgumentParser()    parser.add_argument('-d','--data_list', nargs='+', help='<Required> Set flag', required=True)    parser.add_argument('-o', '--out_dir', type=str, required=True)    parser.add_argument('-a', '--alpha', type=str, required=True, help='Balancing coeficient alpha.')    parser.add_argument('-l', '--languages', nargs='+', required=True, help='List of languages the tokenizer was trained on.')    parser.add_argument('-v', '--vocab_size', type=int, required=True)    parser.add_argument('-t', '--type', type=str, required=False, default="unigram")    parser.add_argument('-c', '--cased', type=bool, default=False)    args = parser.parse_args()    main(args)